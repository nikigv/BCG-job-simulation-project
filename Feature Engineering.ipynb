{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_eda_data.csv')\n",
    "df[\"date_activ\"] = pd.to_datetime(df[\"date_activ\"], format='%Y-%m-%d')\n",
    "df[\"date_end\"] = pd.to_datetime(df[\"date_end\"], format='%Y-%m-%d')\n",
    "df[\"date_modif_prod\"] = pd.to_datetime(df[\"date_modif_prod\"], format='%Y-%m-%d')\n",
    "df[\"date_renewal\"] = pd.to_datetime(df[\"date_renewal\"], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_sales</th>\n",
       "      <th>cons_12m</th>\n",
       "      <th>cons_gas_12m</th>\n",
       "      <th>cons_last_month</th>\n",
       "      <th>date_activ</th>\n",
       "      <th>date_end</th>\n",
       "      <th>date_modif_prod</th>\n",
       "      <th>date_renewal</th>\n",
       "      <th>forecast_cons_12m</th>\n",
       "      <th>...</th>\n",
       "      <th>var_6m_price_off_peak_var</th>\n",
       "      <th>var_6m_price_peak_var</th>\n",
       "      <th>var_6m_price_mid_peak_var</th>\n",
       "      <th>var_6m_price_off_peak_fix</th>\n",
       "      <th>var_6m_price_peak_fix</th>\n",
       "      <th>var_6m_price_mid_peak_fix</th>\n",
       "      <th>var_6m_price_off_peak</th>\n",
       "      <th>var_6m_price_peak</th>\n",
       "      <th>var_6m_price_mid_peak</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24011ae4ebbe3035111d65fa7c15bc57</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>0</td>\n",
       "      <td>54946</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-15</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>4.100838e-05</td>\n",
       "      <td>9.084737e-04</td>\n",
       "      <td>2.086294</td>\n",
       "      <td>99.530517</td>\n",
       "      <td>44.235794</td>\n",
       "      <td>2.086425</td>\n",
       "      <td>9.953056e+01</td>\n",
       "      <td>4.423670e+01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d29c2c54acc38ff3c0614d0a653813dd</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>4660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2016-08-30</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>189.95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.217891e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>1.217891e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>764c75f661154dac3a6c254cd082ea7d</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>47.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.450150e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.450150e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bba03439a292a1e166f80264c16191cb</td>\n",
       "      <td>lmkebamcaaclubfxadlmueccxoimlema</td>\n",
       "      <td>1584</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-30</td>\n",
       "      <td>2016-03-30</td>\n",
       "      <td>2010-03-30</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>240.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149d57cf92fc41cf94415803a877cb4b</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>4425</td>\n",
       "      <td>0</td>\n",
       "      <td>526</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>445.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>2.896760e-06</td>\n",
       "      <td>4.860000e-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>2.896760e-06</td>\n",
       "      <td>4.860000e-10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                     channel_sales  \\\n",
       "0  24011ae4ebbe3035111d65fa7c15bc57  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "1  d29c2c54acc38ff3c0614d0a653813dd                           MISSING   \n",
       "2  764c75f661154dac3a6c254cd082ea7d  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "3  bba03439a292a1e166f80264c16191cb  lmkebamcaaclubfxadlmueccxoimlema   \n",
       "4  149d57cf92fc41cf94415803a877cb4b                           MISSING   \n",
       "\n",
       "   cons_12m  cons_gas_12m  cons_last_month date_activ   date_end  \\\n",
       "0         0         54946                0 2013-06-15 2016-06-15   \n",
       "1      4660             0                0 2009-08-21 2016-08-30   \n",
       "2       544             0                0 2010-04-16 2016-04-16   \n",
       "3      1584             0                0 2010-03-30 2016-03-30   \n",
       "4      4425             0              526 2010-01-13 2016-03-07   \n",
       "\n",
       "  date_modif_prod date_renewal  forecast_cons_12m  ...  \\\n",
       "0      2015-11-01   2015-06-23               0.00  ...   \n",
       "1      2009-08-21   2015-08-31             189.95  ...   \n",
       "2      2010-04-16   2015-04-17              47.96  ...   \n",
       "3      2010-03-30   2015-03-31             240.04  ...   \n",
       "4      2010-01-13   2015-03-09             445.75  ...   \n",
       "\n",
       "   var_6m_price_off_peak_var  var_6m_price_peak_var  \\\n",
       "0                   0.000131           4.100838e-05   \n",
       "1                   0.000003           1.217891e-03   \n",
       "2                   0.000004           9.450150e-08   \n",
       "3                   0.000003           0.000000e+00   \n",
       "4                   0.000011           2.896760e-06   \n",
       "\n",
       "   var_6m_price_mid_peak_var  var_6m_price_off_peak_fix  \\\n",
       "0               9.084737e-04                   2.086294   \n",
       "1               0.000000e+00                   0.009482   \n",
       "2               0.000000e+00                   0.000000   \n",
       "3               0.000000e+00                   0.000000   \n",
       "4               4.860000e-10                   0.000000   \n",
       "\n",
       "   var_6m_price_peak_fix  var_6m_price_mid_peak_fix var_6m_price_off_peak  \\\n",
       "0              99.530517                  44.235794              2.086425   \n",
       "1               0.000000                   0.000000              0.009485   \n",
       "2               0.000000                   0.000000              0.000004   \n",
       "3               0.000000                   0.000000              0.000003   \n",
       "4               0.000000                   0.000000              0.000011   \n",
       "\n",
       "   var_6m_price_peak  var_6m_price_mid_peak  churn  \n",
       "0       9.953056e+01           4.423670e+01      1  \n",
       "1       1.217891e-03           0.000000e+00      0  \n",
       "2       9.450150e-08           0.000000e+00      0  \n",
       "3       0.000000e+00           0.000000e+00      0  \n",
       "4       2.896760e-06           4.860000e-10      0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature engineering\n",
    "\n",
    "### Difference between off-peak prices in December and preceding January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'price_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m price_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m price_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(price_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_date\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m price_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'price_data.csv'"
     ]
    }
   ],
   "source": [
    "price_df = pd.read_csv('price_data.csv')\n",
    "price_df[\"price_date\"] = pd.to_datetime(price_df[\"price_date\"], format='%Y-%m-%d')\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group off-peak prices by companies and month\n",
    "monthly_price_by_id = price_df.groupby(['id', 'price_date']).agg({'price_off_peak_var': 'mean', 'price_off_peak_fix': 'mean'}).reset_index()\n",
    "\n",
    "# Get january and december prices\n",
    "jan_prices = monthly_price_by_id.groupby('id').first().reset_index()\n",
    "dec_prices = monthly_price_by_id.groupby('id').last().reset_index()\n",
    "\n",
    "# Calculate the difference\n",
    "diff = pd.merge(dec_prices.rename(columns={'price_off_peak_var': 'dec_1', 'price_off_peak_fix': 'dec_2'}), jan_prices.drop(columns='price_date'), on='id')\n",
    "diff['offpeak_diff_dec_january_energy'] = diff['dec_1'] - diff['price_off_peak_var']\n",
    "diff['offpeak_diff_dec_january_power'] = diff['dec_2'] - diff['price_off_peak_fix']\n",
    "diff = diff[['id', 'offpeak_diff_dec_january_energy','offpeak_diff_dec_january_power']]\n",
    "diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, diff, on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Average price changes across periods\n",
    "\n",
    "We can now enhance the feature that our colleague made by calculating the average price changes across individual periods, instead of the entire year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average prices per period by company\n",
    "mean_prices = price_df.groupby(['id']).agg({\n",
    "    'price_off_peak_var': 'mean', \n",
    "    'price_peak_var': 'mean', \n",
    "    'price_mid_peak_var': 'mean',\n",
    "    'price_off_peak_fix': 'mean',\n",
    "    'price_peak_fix': 'mean',\n",
    "    'price_mid_peak_fix': 'mean'    \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean difference between consecutive periods\n",
    "mean_prices['off_peak_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices['price_peak_var']\n",
    "mean_prices['peak_mid_peak_var_mean_diff'] = mean_prices['price_peak_var'] - mean_prices['price_mid_peak_var']\n",
    "mean_prices['off_peak_mid_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices['price_mid_peak_var']\n",
    "mean_prices['off_peak_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices['price_peak_fix']\n",
    "mean_prices['peak_mid_peak_fix_mean_diff'] = mean_prices['price_peak_fix'] - mean_prices['price_mid_peak_fix']\n",
    "mean_prices['off_peak_mid_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices['price_mid_peak_fix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'id', \n",
    "    'off_peak_peak_var_mean_diff',\n",
    "    'peak_mid_peak_var_mean_diff', \n",
    "    'off_peak_mid_peak_var_mean_diff',\n",
    "    'off_peak_peak_fix_mean_diff', \n",
    "    'peak_mid_peak_fix_mean_diff', \n",
    "    'off_peak_mid_peak_fix_mean_diff'\n",
    "]\n",
    "df = pd.merge(df, mean_prices[columns], on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature may be useful because it adds more granularity to the existing feature that my colleague found to be useful. Instead of looking at differences across an entire year, we have now created features that look at mean average price differences across different time periods (`off_peak`, `peak`, `mid_peak`). The dec-jan feature may reveal macro patterns that occur over an entire year, whereas inter-time-period features may reveal patterns on a micro scale between months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Max price changes across periods and months\n",
    "\n",
    "looking at the maximum change in prices across periods and months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average prices per period by company\n",
    "mean_prices_by_month = price_df.groupby(['id', 'price_date']).agg({\n",
    "    'price_off_peak_var': 'mean', \n",
    "    'price_peak_var': 'mean', \n",
    "    'price_mid_peak_var': 'mean',\n",
    "    'price_off_peak_fix': 'mean',\n",
    "    'price_peak_fix': 'mean',\n",
    "    'price_mid_peak_fix': 'mean'    \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean difference between consecutive periods\n",
    "mean_prices_by_month['off_peak_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - mean_prices_by_month['price_peak_var']\n",
    "mean_prices_by_month['peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_peak_var'] - mean_prices_by_month['price_mid_peak_var']\n",
    "mean_prices_by_month['off_peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - mean_prices_by_month['price_mid_peak_var']\n",
    "mean_prices_by_month['off_peak_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - mean_prices_by_month['price_peak_fix']\n",
    "mean_prices_by_month['peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_peak_fix'] - mean_prices_by_month['price_mid_peak_fix']\n",
    "mean_prices_by_month['off_peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - mean_prices_by_month['price_mid_peak_fix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maximum monthly difference across time periods\n",
    "max_diff_across_periods_months = mean_prices_by_month.groupby(['id']).agg({\n",
    "    'off_peak_peak_var_mean_diff': 'max',\n",
    "    'peak_mid_peak_var_mean_diff': 'max',\n",
    "    'off_peak_mid_peak_var_mean_diff': 'max',\n",
    "    'off_peak_peak_fix_mean_diff': 'max',\n",
    "    'peak_mid_peak_fix_mean_diff': 'max',\n",
    "    'off_peak_mid_peak_fix_mean_diff': 'max'\n",
    "}).reset_index().rename(\n",
    "    columns={\n",
    "        'off_peak_peak_var_mean_diff': 'off_peak_peak_var_max_monthly_diff',\n",
    "        'peak_mid_peak_var_mean_diff': 'peak_mid_peak_var_max_monthly_diff',\n",
    "        'off_peak_mid_peak_var_mean_diff': 'off_peak_mid_peak_var_max_monthly_diff',\n",
    "        'off_peak_peak_fix_mean_diff': 'off_peak_peak_fix_max_monthly_diff',\n",
    "        'peak_mid_peak_fix_mean_diff': 'peak_mid_peak_fix_max_monthly_diff',\n",
    "        'off_peak_mid_peak_fix_mean_diff': 'off_peak_mid_peak_fix_max_monthly_diff'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'id',\n",
    "    'off_peak_peak_var_max_monthly_diff',\n",
    "    'peak_mid_peak_var_max_monthly_diff',\n",
    "    'off_peak_mid_peak_var_max_monthly_diff',\n",
    "    'off_peak_peak_fix_max_monthly_diff',\n",
    "    'peak_mid_peak_fix_max_monthly_diff',\n",
    "    'off_peak_mid_peak_fix_max_monthly_diff'\n",
    "]\n",
    "\n",
    "df = pd.merge(df, max_diff_across_periods_months[columns], on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the perspective of a PowerCo client, I believe calculating the maximum price change between months and time periods could be a valuable feature to include. For a Utilities customer, I understand how frustrating sudden price changes can be. A significant increase in prices within a short time span would likely prompt me to explore alternative providers for better deals. Hence, incorporating this feature could provide valuable insights into customer behavior and help us predict churn more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tenure\n",
    "\n",
    "How long a company has been a client of PowerCo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tenure'] = ((df['date_end'] - df['date_activ'])/ np.timedelta64(1, 'Y')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['tenure']).agg({'churn': 'mean'}).sort_values(by='churn', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's apparent that companies with a tenure of 4 months or less exhibit a significantly higher churn rate compared to those with longer tenures. What's particularly intriguing is the notable 4% increase in churn likelihood between companies at the 4-month mark compared to those at 5 months. This disparity suggests that reaching the 4-month tenure milestone may signify a critical juncture in retaining customers for the long term.\n",
    "\n",
    "This insight underscores the importance of tenure duration in predicting churn behavior and highlights the potential value of including this feature in our modeling efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming dates into months\n",
    "\n",
    "- months_activ = Number of months active until reference date (Jan 2016)\n",
    "- months_to_end = Number of months of the contract left until reference date (Jan 2016)\n",
    "- months_modif_prod = Number of months since last modification until reference date (Jan 2016)\n",
    "- months_renewal = Number of months since last renewal until reference date (Jan 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_months(reference_date, df, column):\n",
    "    \"\"\"\n",
    "    Input a column with timedeltas and return months\n",
    "    \"\"\"\n",
    "    time_delta = reference_date - df[column]\n",
    "    months = (time_delta / np.timedelta64(1, 'M')).astype(int)\n",
    "    return months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reference date\n",
    "reference_date = datetime(2016, 1, 1)\n",
    "\n",
    "# Create columns\n",
    "df['months_activ'] = convert_months(reference_date, df, 'date_activ')\n",
    "df['months_to_end'] = -convert_months(reference_date, df, 'date_end')\n",
    "df['months_modif_prod'] = convert_months(reference_date, df, 'date_modif_prod')\n",
    "df['months_renewal'] = convert_months(reference_date, df, 'date_renewal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformed datetime objects into predictive features, creating:\n",
    "\n",
    "- \"Months_Activ\" to assess loyalty based on tenure.\n",
    "\n",
    "- \"Months_to_End\" to understand churn timing with contract expiration.\n",
    "\n",
    "- \"Months_Modif_Prod\" to measure engagement via recent contract updates.\n",
    "\n",
    "- \"Months_Renewal\" to gauge commitment through renewal history.\n",
    "\n",
    "These features enhance predictive modeling by capturing vital client behaviors and churn dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need the datetime columns that we used for feature engineering, so we can drop them\n",
    "remove = [\n",
    "    'date_activ',\n",
    "    'date_end',\n",
    "    'date_modif_prod',\n",
    "    'date_renewal'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=remove)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Boolean data\n",
    "\n",
    "#### has_gas\n",
    "\n",
    "We simply want to transform this column from being categorical to being a binary flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_gas'] = df['has_gas'].replace(['t', 'f'], [1, 0])\n",
    "df.groupby(['has_gas']).agg({'churn': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The presence of gas purchases indicates loyalty, as customers with multiple products are more inclined to remain with PowerCo. Consequently, customers without gas purchases exhibit nearly 2% higher churn likelihood. Thus, gas purchase serves as a valuable feature in predicting churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming categorical data\n",
    "\n",
    "#### channel_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into categorical type\n",
    "df['channel_sales'] = df['channel_sales'].astype('category')\n",
    "\n",
    "# Let's see how many categories are within this column\n",
    "df['channel_sales'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 8 categories, the last 3 categories have notably low occurrences: 11, 3, and 2 respectively, out of a dataset with approximately 14,000 rows. Consequently, the corresponding dummy variables for these categories would largely consist of zeros, offering minimal predictive power. Therefore, to streamline the model and prioritize informative features, we have decided to drop these 3 dummy variables from consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['channel_sales'], prefix='channel')\n",
    "df = df.drop(columns=['channel_sddiedcslfslkckwlfkdpoeeailfpeds', 'channel_epumfxlbckeskwekxbiuasklxalciiuu', 'channel_fixdbufsefwooaasfcxdxadsiekoceaa'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### origin_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into categorical type\n",
    "df['origin_up'] = df['origin_up'].astype('category')\n",
    "\n",
    "# Let's see how many categories are within this column\n",
    "df['origin_up'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following a similar pattern observed in the channel_sales feature, the last 3 categories in the output display considerably low frequencies. Therefore, after generating dummy variables for these categories, we will proceed to remove them from the feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['origin_up'], prefix='origin_up')\n",
    "df = df.drop(columns=['origin_up_MISSING', 'origin_up_usapbepcfoloekilkwsdiboslwaxobdp', 'origin_up_ewxeelcelemmiwuafmddpobolfuxioce'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming numerical data\n",
    "\n",
    "to treat skewed variables we opt for the 'Logarithm' transformation for positively skewed feature \n",
    "\n",
    "<b>Note:</b> We cannot apply log to a value of 0, so we will add a constant of 1 to all the values\n",
    "\n",
    "First I want to see the statistics of the skewed features, so that we can compare before and after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = [\n",
    "    'cons_12m', \n",
    "    'cons_gas_12m', \n",
    "    'cons_last_month',\n",
    "    'forecast_cons_12m', \n",
    "    'forecast_cons_year', \n",
    "    'forecast_discount_energy',\n",
    "    'forecast_meter_rent_12m', \n",
    "    'forecast_price_energy_off_peak',\n",
    "    'forecast_price_energy_peak', \n",
    "    'forecast_price_pow_off_peak'\n",
    "]\n",
    "\n",
    "df[skewed].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the standard deviation for most of these features is quite high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log10 transformation\n",
    "df[\"cons_12m\"] = np.log10(df[\"cons_12m\"] + 1)\n",
    "df[\"cons_gas_12m\"] = np.log10(df[\"cons_gas_12m\"] + 1)\n",
    "df[\"cons_last_month\"] = np.log10(df[\"cons_last_month\"] + 1)\n",
    "df[\"forecast_cons_12m\"] = np.log10(df[\"forecast_cons_12m\"] + 1)\n",
    "df[\"forecast_cons_year\"] = np.log10(df[\"forecast_cons_year\"] + 1)\n",
    "df[\"forecast_meter_rent_12m\"] = np.log10(df[\"forecast_meter_rent_12m\"] + 1)\n",
    "df[\"imp_cons\"] = np.log10(df[\"imp_cons\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[skewed].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that for the majority of the features, their standard deviation is much lower after transformation. This is a good thing, it shows that these features are more stable and predictable now.\n",
    "\n",
    "Let's quickly check the distributions of some of these features too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, figsize=(18, 20))\n",
    "# Plot histograms\n",
    "sns.distplot((df[\"cons_12m\"].dropna()), ax=axs[0],color='g')\n",
    "sns.distplot((df[df[\"has_gas\"]==1][\"cons_gas_12m\"].dropna()), ax=axs[1], color='orange')\n",
    "sns.distplot((df[\"cons_last_month\"].dropna()), ax=axs[2], color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "High correlations between features may suggest redundant information, challenging the assumption of feature independence in predictive models. Ideally, features should correlate strongly with the target variable (churn) but have minimal correlation with independent variables. Let's explore feature correlations to understand their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation\n",
    "plt.figure(figsize=(45, 45))\n",
    "sns.heatmap(\n",
    "    correlation, \n",
    "    xticklabels=correlation.columns.values,\n",
    "    yticklabels=correlation.columns.values, \n",
    "    annot=True, \n",
    "    annot_kws={'size': 12}\n",
    ")\n",
    "# Axis ticks size\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will remove two variables(num_years_antig and forecast_cons_year) which exhibit a high correlation with other independent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['num_years_antig', 'forecast_cons_year'])\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "152bf6e7dc8ee53edb5af21dc1a8faeab7f134840808a94079ed98d91ece7e0c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
